---
layout: post
title: Natural Language Processing Books
date: 2023-05-20 00:00:00
tags: books resources
categories: [blog]
comments: true
disqus_identifier: 202305200
preview_pic: /assets/images/resources/foundations_of_statistical_nlp.jpg
---

Learning about Natural Language Processing is a continuous task, which appears not to have an end, but, there is always a starting point where one learns the problem definitions and the common algorithms to solve them. In this post, I will share some of the books that I used during my path to learn about Natural Language Processing. I think most of these books are a good starting point to learn about Natural Language Processing and how to apply Machine learning to NLP tasks. Most of these books and tutorials are nice to have around so that you can quickly clarify any doubts or review how a certain algorithm or technique works. I personally like to have them at hand :)

<br>

#### [__Neural Network Methods for Natural Language Processing__](https://www.morganclaypool.com/doi/abs/10.2200/S00762ED1V01Y201703HLT037)

<p>
<a href="https://www.morganclaypool.com/doi/abs/10.2200/S00762ED1V01Y201703HLT037"><img style="float: left; margin: 0px 15px 15px 0px;" alt="Neural Network Methods for Natural Language Processing" src="/assets/images/resources/neural_networks_for_nlp.jpg" height="220" width="165"></a>
I would recommend this book to anyone who is not a beginner and wants to make the jump from "classic NLP", i.e.: exploring hand-made features together with SVMs, HMM, CRFs and alike and wants to have an overview of how neural networks can be applied to several NLP tasks. It takes a very good overview of how neural networks took over NLP from 2012/2013 up until 2016. It first covers the basics of neural networks and then slowly goes explains how they can be used to solve different NLP tasks, starting with more simple ones and going into specific ones, it contains many references to papers that at the time made the first breakthroughs on using neural networks for NLP.
</p>

<br>

---

<br>

#### [__Deep Learning with Python__](https://www.manning.com/books/deep-learning-with-python)

<p>
<a href="https://www.manning.com/books/deep-learning-with-python">
<img style="float: left; margin: 0px 15px 15px 0px;" alt="Deep Learning with Python" src="/assets/images/resources/Chollet-DLP-HI.png" height="220" width="150"></a>

This is a hands-on book on Keras written by the creator of Keras himself François Chollet. It starts with a quick overview of what Machine Learning explaining the basics in such a way that the non-mathematical person can understand the concepts behind it. I then continues through with examples of simple regressions problems and also binary and multi-class classification.
It contains chapters dedicated to image and another dedicated to text and sequence processing, and also more advanced topics such as neural network fine-tuning and generative models. I would suggest this book to programmers who want to get started with neural networks, especially using Keras.
</p>

<br>

---

<br>


#### [__Deep Learning__](https://www.deeplearningbook.org/)

<p>
<a href="https://www.deeplearningbook.org/">
<img style="float: left; margin: 0px 15px 15px 0px;" alt="Deep learning" src="/assets/images/resources/deep_learning.jpg" height="200" width="150"></a>
I would dare to say that this is the current neural networks "bible", it goes into deep detail on every aspect of Machine Learning and more specifically on Neural Networks. It really starts from scratch, the first four chapters are entirely dedicated to topics such as Linear Algebra, Machine Learning concepts. Then it details neural networks, starting with forward neural networks, and introducing the concepts of regularisation and optimisation on neural networks, then it describes the different architectures, i.e: convolutional networks and sequence modelling. The final part of the book goes into advanced topics on deep learning, such as auto-encoders and deep generative models. This book goes into very detail explaining every architecture of different neural networks. If you really want to go into the details this is the one.
</p>


<br>

---

<br>


#### [__Foundations of Natural Language Processing__](https://nlp.stanford.edu/fsnlp/)


<p>
<a href="https://nlp.stanford.edu/fsnlp/">
<img style="float: left; margin: 0px 15px 15px 0px;" alt="Foundations of Natural Language Processing" src="/assets/images/resources/foundations_of_statistical_nlp.jpg" height="200" width="150"></a>
This book is a classic :) It was published in 1999 so all the content is before the Machine Learning hype during the 2000s and later (and currently) the Deep Learning hype. All the methods are very clearly detailed and explained. It starts with simple but very important concepts related to word counts, collocations, Zipf's Law, etc. It dives into mathematical foundations around probability and information theory and shows how one can use Markov Models for sequence tagging. It also introduces the concepts of Probabilistic Context-Free Grammars and Parsing.
</p>

<br>

---

<br>

#### [__Introduction to Information Retrieval__](https://nlp.stanford.edu/IR-book/information-retrieval-book.html)

<p>
<a href="https://nlp.stanford.edu/IR-book/information-retrieval-book.html"><img style="float: left; margin: 0px 15px 15px 0px;" alt="Introduction to Information Retrieval" src="/assets/images/resources/information_retrieval.jpg" height="200" width="150"></a>
I started my journey in NLP and Machine Learning back in 2007 with Information Retrieval, more precisely Geographic Information Retrieval. This book is a great start before you jump into more advanced topics. It covers subjects related to indexing and retrieving documents and goes into great detail on the algorithms and techniques behind the scenes of a document search engine. It also covers some machine learning algorithms like Naïve Bayes and Support Vector Machines, and how they can be used for document retrieval.
</p>

<br>

---

<br>

#### [__The Elements of Statistical Learning__](http://web.stanford.edu/~hastie/ElemStatLearn/)

<p>
<a href="http://web.stanford.edu/~hastie/ElemStatLearn/">
<img style="float: left; margin: 0px 15px 15px 0px;" alt="The Elements of Statistical Learning" src="/assets/images/resources/elements_of_statistical_learning.jpg" height="200" width="150"></a>
This is, I would say one of the main references for machine learning from the statistical point of view. The book is also kind of a classic going in its 2nd Edition. It's considered a very complete reference book, one of the authors is Prof. Tibshirani, who proposed the LASSO regularisation technique. It uses some mathematical statistics avoiding complicated proofs to explain machine learning concepts. It describes popular machine algorithms such as Logistic Regressions, SVMs, Random Forests, etc.; but each is developed only after the appropriate statistical framework has already been introduced.
</p>

<br>

---

<br>

#### [__Speech and Language Processing__](https://web.stanford.edu/~jurafsky/slp3/)

<p>
<a href="https://web.stanford.edu/~jurafsky/slp3/">
<img style="float: left; margin: 0px 15px 15px 0px;" alt="Speech and Language Processing" src="/assets/images/resources/speech_and_lang_process.jpg" height="200" width="150"></a>

This is another very complete book which goes well beyond theory by also presenting some practical examples, one can see this book as an extension to the <i>Foundations of Natural Language Processing</i>. It contains besides the description of statistical methods applied to NLP, also some chapters on Phonology, Formal Grammar, Parsing and other Language-related topics. It also contains, as the title suggests chapters dedicated only to Speech, which might be interesting for these interested in speech-related applications, such as text-to-speech. I would say this is an interesting book for those also interested in the computational linguistics aspects of Natural Language Processing.

</p>