---
layout: post
title: Reviewing the Transformer Architecture
date: 2023-06-27 00:00:00
tags: Transformers NLP
categories: [blog]
comments: true
disqus_identifier: 20230627
preview_pic: /assets/images/2023-06-27-transformer_arch.png
---

The transformer model was a game-changer in NLP and also in other domains. I wanted to review and understand in detail the Transformer architecture which become the base framework for many other models that we have available today to solve a vast number of NLP tasks, but also other tasks outside the NLP domain.

<!--
https://www.youtube.com/watch?v=ptuGllU5SQQ&list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ&index=9&ab_channel=StanfordOnline

"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding": https://aclanthology.org/N19-1423.pdf
-->



A transformer is a particular attention-based model, more specifically a self-attention model. It was initially proposed as another algorithm to perform Machine Translation overcoming the limitations of the recurrent neural networks by being based solely on attention mechanisms, dispensing with recurrence and convolutions entirely


<!--
- unrolled left-to-right, essentially one direction
- O(sequence length) steps for distant word pairs to interact means:
- The chef who .................. was 
- info of "chef" was gone through O(seq) many layers
- in between lengths there are other words/computations before "chef" and "was" that can interact
- hard to learn long-distance dependencies (because of gradient problems)
- lack of parallelizability: GPU can perform a bunch of independent computations at once!
- RNN hidden states can't be computed in full before past RNN hidden states have been computed
-->



- Encoders -> Decoders








## __Self-Attention__:

<!-- link to the post about attention from the decoder to the encoder -->

Think about attention within a sentence, the Transformer model relies solely on the use of self-attention, i.e.: the representation of a sequence (or sentence) is computed by relating different words in the same sequence. It compares each word with every other word in the sequence (or sentence) trying to understand which ones are more related to each other.

### first step
We create three vectors from each of the encoder’s input vectors (i.e., the embedding of each word): a Query vector, a Key vector, and a Value vector. These vectors are generated by multiplying the embedding vector of a word by three matrices.

__TODO__: where do the matrices come from, randomly initialized? and tuned during learning?
<!-- that we trained during the training process.-->

<!-- verbatim -->
Notice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have a dimensionality of 512. They don’t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant.


### second step
<!-- verbatim -->
The second step in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.

The score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.


### third step and fourth
The third and fourth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper – 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1.


### fifth and sixth
The fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).

The sixth step is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).


### Matrix Calculation of Self-Attention
see image: http://jalammar.github.io/illustrated-transformer/





<!-- 
https://d2l.ai/chapter_attention-mechanisms-and-transformers/queries-keys-values.html
We call our particular attention "Scaled Dot-Product Attention" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. 
We compute the dot products of the query with all keys, divide each by sqrt(d_{k}), and apply a softmax function to obtain the weights on the values
-->




- the number of unparallelizabe operations does not increase with a sequence length
- max interaction distance: O(1), all words interact at every layer



- everything is looking within itself
- queries, keys, values
- q is a vector
- keys also a vector
- values also a vector
- all in the same dimension
- q, k, v are drawn from the same source of information
- dot product self-attention operations (See details)
	- compute key-query affinities
	- compute attention weights from affinities
	- ....


Self-Attention as an NLP building block:
========================================
	- doesn't have an inherent notion of order
		- we need to have a sequence order
		- we need to encode the order of the sentence in our keys, queries and a values 
		- position representation vectors learned from scratch

	- no nonlinearities for deep learning, it's all just weighted averages
		- attention_within_a_sentence_4 (image)
		- apply the same feedforward network to each self-attention output
	
	- don't look at the future
		- mask the future in self-attention
		- attention_within_a_sentence_5 (image)
		

	- Are matrices K, Q, V parameters to be learned?



## __Multi-Head Attention__

What was described before computes attention






##

Transformer model
=================
	- Encoder, Decoder Blocks
	- attention_within_a_sentence_6
	- How do we get the k, q, and v vectors from a single word embedding?
	- Multi-headed attention: attend multiple places in a single layer
	- Tricks to improve training
		- Residual connections
		- Layer normalization
		- Scaling the dot product
		- all these above help improve the training process

	- Key, Query, Value attention
		- self-attention is when keys, queries and values come from the same source, the transformer does this in a particular way
		- attention_within_a_sentence_7
		- attention_within_a_sentence_8

	- Multi-headed attention
		- attention_within_a_sentence_9
		- attention_within_a_sentence_10

	- Residual Connections
	- Layer Normalization (help models train faster)
	- Scaled dot-product attention

	- Transformer Encoder Block: 
		attention_within_a_sentence_11

	- Transformer Decoder Block:
		attention_within_a_sentence_12
		- attend only to the last encoder block
		- Masked Multi-head self-attention
		- Multi-Head Cross-Attention

	- Drawbacks
		- Quadratic computing in self-attention
		- Computing all pairs of interactions means our computation grows quadratically with seq. length
		- Quadratic computation as a function of sequence length
	- solutions:
		- map k, v, q to a small dimensional space before computing the dot product
		- replace all-pairs interactions with other interactions: (BigBird paper)
			- local windows
			- look at everything: average the whole sentence in a single vector
			- random interactions
		










## __Subword Modelling__

- Byte-pair encoding




## __Transformer pre-training__


	




## __BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding__

<!-- https://aclanthology.org/N19-1423.pdf -->

How is BERT different from the original transformer architecture?

(https://ai.stackexchange.com/questions/23221/how-is-bert-different-from-the-original-transformer-architecture)

The original transformer, proposed in the paper Attention is all you need (2017), is an encoder-decoder-based neural network that is mainly characterized by the use of the so-called attention (i.e. a mechanism that determines the importance of words to other words in a sentence or which words are more likely to come together) and the non-use of recurrent connections (or recurrent neural networks) to solve tasks that involve sequences (or sentences), even though RNN-based systems were becoming the standard practice to solve natural language processing (NLP) or understanding (NLU) tasks. Hence the name of the paper "Attention is all you need", i.e. you only need attention and you don't need recurrent connections to solve NLP tasks.	

Both the encoder-decoder architecture and the attention mechanism are not novel proposals. In fact, previous neural network architectures to solve many NLP tasks, such as machine translation, had already used these mechanisms (for example, take a look at this paper). The novelty of the transformer and this cited paper is that it shows that we can simply use attention to solve tasks that involve sequences (such as machine translation) and we do not need recurrent connections, which is an advantage, given that recurrent connections can hinder the parallelization of the training process.

The original transformed architecture is depicted in figure 1 of the cited paper. Both the encoder and decoder are composed of



What is BERT?

BERT stands for Bidirectional Encoder Representations from Transformers, so, as the name suggests, it is a way of learning representations of a language that uses a transformer, specifically, the encoder part of the transformer.

What is the difference between the transformer and BERT?

BERT is a language model, i.e. it represents the statistical relationships of the words in a language, i.e. which words are more likely to come after a given word or phrase. Hence the part Representations in its name, Bidirectional Encoder Representations from Transformers.


BERT can be trained in an unsupervised way for representation learning, and then we can fine-tune BERT on the so-called downstream tasks in a supervised fashion (i.e. transfer learning). There are pre-trained versions of BERT that can be already fine-tuned (e.g. this one) and used to solve your specific supervised learning task. You can play with this TensorFlow tutorial to use a pre-trained BERT model.


On the other hand, the original transformed was not originally conceived to be a language model, but to solve sequence transduction tasks (i.e. converting one sequence to another, such as machine translation) without recurrent connections (or convolutions) but only attention.


BERT is only an encoder, while the original transformer is composed of an encoder and decoder. Given that BERT uses an encoder that is very similar to the original encoder of the transformer, we can say that BERT is a transformer-based model. So, BERT does not use recurrent connections, but only attention and feed-forward layers. There are other transformed-based neural networks that use only the decoder part of the transformer, for example, the GPT model. -> https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf



BERT uses different hyper-parameters than the ones used in Attention is all you need to achieve the best performance. For example, it uses 12 and 16 "attention heads" (please, read the transformer paper to know more about these "attention heads") rather than 8 (although in the original transformer paper the authors experimented with a different number of heads).

BERT also uses segment embeddings, while the original transformer only uses word embeddings and positional encodings.



When to use BERT and the transformer?
Although I never used them, I would say that you want to use BERT whenever you want to solve an NLP task in a supervised fashion, but your labeled training dataset is not big enough to achieve good performance. In that case, you start with a pre-trained BERT model, then fine-tune it with your small labeled dataset. You probably need to add specific layers to BERT to solve your task.





























