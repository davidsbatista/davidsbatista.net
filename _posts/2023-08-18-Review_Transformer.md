---
layout: post
title: Reviewing the Transformer Architecture
date: 2023-08-18 00:00:00
tags: transformers neural-networks
categories: [blog]
comments: true
disqus_identifier: 20230813
preview_pic: /assets/images/2023-08-18-transformer_arch.png
---

The Transformer was a groundbreaking, revolutionary, innovative deep learning architecture that impacted NLP and other domains. I wanted to review and understand in detail the Transformer architecture which became the de-facto framework for many foundational and pre-trained models available today and used to solve a vast number of NLP tasks. This post is a review of the mechanisms that build up the Transformer: self-attention, multi-head attention, cross attention and how these are connected with encoder-decoder architecture.


## __Introduction__

A Transformer is a particular type of attention-based model, more specifically a self-attention model. It was initially proposed as another algorithm to perform Machine Translation overcoming the limitations of the recurrent neural networks by being based solely on the attention mechanism, dispensing entirely recurrence and convolutions.

<!--
- unrolled left-to-right, essentially one direction
- O(sequence length) steps for distant word pairs to interact means:
- The chef who .................. was 
- info of "chef" was gone through O(seq) many layers
- in between lengths there are other words/computations before "chef" and "was" that can interact
- hard to learn long-distance dependencies (because of gradient problems)
- lack of parallelizability: GPU can perform a bunch of independent computations at once!
- RNN hidden states can't be computed in full before past RNN hidden states have been computed
-->


<figure>
  <img style="width: 65%; height: 65%" src="/assets/images/2023-08-18-transformer_svg_arch.svg">
  <figcaption></figcaption>
</figure>


## __Self-Attention__

<!-- verbatim -->

In deep learning, we often use CNNs or RNNs to encode sequences. Now, with attention mechanisms in mind, imagine feeding a sequence of tokens into an attention mechanism such that each token has its own query, keys, and values at each step. 

Here, when computing the value of a token’s representation at the next layer, the token can attend (via its query vector) to each other token (matching based on their key vectors). Using the full set of query-key compatibility scores, we can compute, for each token, a representation by building the appropriate weighted sum over the other tokens. Because each token is attending to each other token (unlike the case where decoder steps attend to encoder steps), such architectures are typically described as self-attention models (Lin et al., 2017, Vaswani et al., 2017), and elsewhere described as intra-attention model (Cheng et al., 2016, Parikh et al., 2016, Paulus et al., 2017). 

Self-Attention Mechanism: The paper introduced the concept of self-attention, enabling the model to weigh the importance of different tokens within a sequence when generating contextual representations. This mechanism allows the model to capture long-range dependencies and efficiently process information across the entire sequence simultaneously.


Think about attention within a sentence, the Transformer model relies solely on the use of self-attention, i.e.: the representation of a sequence (or sentence) is computed by relating different words in the same sequence. It compares each word with every other word in the sequence (or sentence) trying to understand which ones are more related to each other.



<!-- 

### first step
We create three vectors from each of the encoder’s input vectors (i.e., the embedding of each word): a Query vector, a Key vector, and a Value vector. These vectors are generated by multiplying the embedding vector of a word by three matrices.

__TODO__: where do the matrices come from, randomly initialized? and tuned during learning?
that we trained during the training process.

verbatim
Notice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have a dimensionality of 512. They don’t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant.


### second step
verbatim
The second step in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.

The score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.


### third step and fourth
The third and fourth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper – 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1.


### fifth and sixth
The fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).

The sixth step is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).


### Matrix Calculation of Self-Attention
see image: http://jalammar.github.io/illustrated-transformer/



https://d2l.ai/chapter_attention-mechanisms-and-transformers/queries-keys-values.html
We call our particular attention "Scaled Dot-Product Attention" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. 
We compute the dot products of the query with all keys, divide each by sqrt(d_{k}), and apply a softmax function to obtain the weights on the values



- the number of unparallelizabe operations does not increase with a sequence length
- max interaction distance: O(1), all words interact at every layer

- everything is looking within itself
- queries, keys, values
- q is a vector
- keys also a vector
- values also a vector
- all in the same dimension
- q, k, v are drawn from the same source of information
- dot product self-attention operations (See details)
	- compute key-query affinities
	- compute attention weights from affinities
	- ....


Self-Attention as an NLP building block:
========================================
	- doesn't have an inherent notion of order
		- we need to have a sequence order
		- we need to encode the order of the sentence in our keys, queries and a values 
		- position representation vectors learned from scratch

	- no nonlinearities for deep learning, it's all just weighted averages
		- attention_within_a_sentence_4 (image)
		- apply the same feedforward network to each self-attention output
	
	- don't look at the future
		- mask the future in self-attention
		- attention_within_a_sentence_5 (image)
		

	- Are matrices K, Q, V parameters to be learned?

-->

## __Multi-Head Attention__


<!-- verbatim  -->

In practice, given the same set of queries, keys, and values we may want our model to combine knowledge from different behaviours of the same attention mechanism, such as capturing dependencies of various ranges (e.g., shorter-range vs. longer-range) within a sequence. Thus, it may be beneficial to allow our attention mechanism to jointly use different representation subspaces of queries, keys, and values.

The model can capture different types of relationships and learn diverse representations, leading to more expressive and robust contextual embeddings.


<!--
Learnable parameters: weigh matrices for q, k, v

The multi-head attention output is another linear transformation via learnable parameters

see: https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html
-->



## __Positional Encoding__




## __The Transformer Encoder-Decoder__

<!-- 

	- Encoder, Decoder Blocks
	- attention_within_a_sentence_6
	- How do we get the k, q, and v vectors from a single word embedding?
	- Multi-headed attention: attend multiple places in a single layer
	- Tricks to improve training
		- Residual connections
		- Layer normalization
		- Scaling the dot product
		- all these above help improve the training process

	- Key, Query, Value attention
		- self-attention is when keys, queries and values come from the same source, the transformer does this in a particular way
		- attention_within_a_sentence_7
		- attention_within_a_sentence_8

	- Multi-headed attention
		- attention_within_a_sentence_9
		- attention_within_a_sentence_10

	- Residual Connections
	- Layer Normalization (help models train faster)
	- Scaled dot-product attention

	- Transformer Encoder Block: 
		attention_within_a_sentence_11

	- Transformer Decoder Block:
		attention_within_a_sentence_12
		- attend only to the last encoder block
		- Masked Multi-head self-attention
		- Multi-Head Cross-Attention

	- Drawbacks
		- Quadratic computing in self-attention
		- Computing all pairs of interactions means our computation grows quadratically with seq. length
		- Quadratic computation as a function of sequence length
	- solutions:
		- map k, v, q to a small dimensional space before computing the dot product
		- replace all-pairs interactions with other interactions: (BigBird paper)
			- local windows
			- look at everything: average the whole sentence in a single vector
			- random interactions
		
-->









## __Subword Modelling__

- Byte-pair encoding
- Word-piece




## __Large-Scale Pre-training with Transformers__

Transformers can be used in three different modes: 

- __encoder-only__
- __encoder-decoder__
- __decoder-only__


#### __Encoder only__

<!-- verbatim -->
When only the Transformer encoder is used, a sequence of input tokens is converted into the same number of representations that can be further projected into output (e.g., classification). A Transformer encoder consists of self-attention layers, where all input tokens attend to each other.

Since this representation depends on all input tokens, it is further projected into classification labels. This design was inspired by an earlier encoder-only Transformer pre-trained on text: BERT (Bidirectional Encoder Representations from Transformers) 

BERT is pre-trained on text sequences using masked language modelling: input text with randomly masked tokens is fed into a Transformer encoder to predict the masked tokens. As illustrated in Fig. 11.9.1, an original text sequence “I”, “love”, “this”, “red”, “car” is prepended with the “<cls>” token, and the “<mask>” token randomly replaces “love”; then the cross-entropy loss between the masked token “love” and its prediction is to be minimised during pre-training. Note that there is no constraint in the attention pattern of Transformer encoders (right of Fig. 11.9.1) so all tokens can attend to each other. Thus, the prediction of “love” depends on input tokens before and after it in the sequence. This is why BERT is a “bidirectional encoder”. Without the need for manual labelling, large-scale text data from books and Wikipedia can be used for pre-training BERT.


#### __Encoder-Decoder__

<!-- verbatim -->
Since a Transformer encoder converts a sequence of input tokens into the same number of output representations, the encoder-only mode cannot generate a sequence of arbitrary length like in machine translation. As originally proposed for machine translation, the Transformer architecture can be outfitted with a decoder that auto-regressively predicts the target sequence of arbitrary length, token by token, conditional on both encoder output and decoder output: 

- (i) for conditioning on encoder output, encoder-decoder cross-attention (multi-head attention of decoder in Fig. 11.7.1) allows target tokens to attend to all input tokens

- (ii) conditioning on decoder output is achieved by so-called causal attention (this name is common in the literature but is misleading as it has little connection to the proper study of causality) pattern (masked multi-head attention of decoder in Fig. 11.7.1), where any target token can only attend to past and present tokens in the target sequence

BART (Lewis et al., 2019) and T5 (Raffel et al., 2020) are two concurrently proposed encoder-decoder Transformers pre-trained on large-scale text corpora. Both attempt to reconstruct original text in their pre-training objectives, while the former emphasises noising input (e.g., masking, deletion, permutation, and rotation) and the latter highlights multitask unification with comprehensive ablation studies.


In the encoder-decoder cross-attention (upper rectangle), each target token attends to all input tokens; In the decoder self-attention (upper triangle), each target token attends to present and past target tokens only (causal)


#### __Decoder-Only__

Note that the attention pattern in the Transformer decoder enforces that each token can only attend to its past tokens (future tokens cannot be attended to because they have not yet been chosen).


<!--
## __BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding__

<!-- https://aclanthology.org/N19-1423.pdf -->


<!-- 
How is BERT different from the original transformer architecture?

(https://ai.stackexchange.com/questions/23221/how-is-bert-different-from-the-original-transformer-architecture)

The original transformer, proposed in the paper Attention is all you need (2017), is an encoder-decoder-based neural network that is mainly characterized by the use of the so-called attention (i.e. a mechanism that determines the importance of words to other words in a sentence or which words are more likely to come together) and the non-use of recurrent connections (or recurrent neural networks) to solve tasks that involve sequences (or sentences), even though RNN-based systems were becoming the standard practice to solve natural language processing (NLP) or understanding (NLU) tasks. Hence the name of the paper "Attention is all you need", i.e. you only need attention and you don't need recurrent connections to solve NLP tasks.	

Both the encoder-decoder architecture and the attention mechanism are not novel proposals. In fact, previous neural network architectures to solve many NLP tasks, such as machine translation, had already used these mechanisms (for example, take a look at this paper). The novelty of the transformer and this cited paper is that it shows that we can simply use attention to solve tasks that involve sequences (such as machine translation) and we do not need recurrent connections, which is an advantage, given that recurrent connections can hinder the parallelization of the training process.

The original transformed architecture is depicted in figure 1 of the cited paper. Both the encoder and decoder are composed of



What is BERT?

BERT stands for Bidirectional Encoder Representations from Transformers, so, as the name suggests, it is a way of learning representations of a language that uses a transformer, specifically, the encoder part of the transformer.

What is the difference between the transformer and BERT?

BERT is a language model, i.e. it represents the statistical relationships of the words in a language, i.e. which words are more likely to come after a given word or phrase. Hence the part Representations in its name, Bidirectional Encoder Representations from Transformers.


BERT can be trained in an unsupervised way for representation learning, and then we can fine-tune BERT on the so-called downstream tasks in a supervised fashion (i.e. transfer learning). There are pre-trained versions of BERT that can be already fine-tuned (e.g. this one) and used to solve your specific supervised learning task. You can play with this TensorFlow tutorial to use a pre-trained BERT model.


On the other hand, the original transformed was not originally conceived to be a language model, but to solve sequence transduction tasks (i.e. converting one sequence to another, such as machine translation) without recurrent connections (or convolutions) but only attention.


BERT is only an encoder, while the original transformer is composed of an encoder and decoder. Given that BERT uses an encoder that is very similar to the original encoder of the transformer, we can say that BERT is a transformer-based model. So, BERT does not use recurrent connections, but only attention and feed-forward layers. There are other transformed-based neural networks that use only the decoder part of the transformer, for example, the GPT model. -> https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf



BERT uses different hyper-parameters than the ones used in Attention is all you need to achieve the best performance. For example, it uses 12 and 16 "attention heads" (please, read the transformer paper to know more about these "attention heads") rather than 8 (although in the original transformer paper the authors experimented with a different number of heads).

BERT also uses segment embeddings, while the original transformer only uses word embeddings and positional encodings.



When to use BERT and the transformer?
Although I never used them, I would say that you want to use BERT whenever you want to solve an NLP task in a supervised fashion, but your labeled training dataset is not big enough to achieve good performance. In that case, you start with a pre-trained BERT model, then fine-tune it with your small labeled dataset. You probably need to add specific layers to BERT to solve your task.

-->

### __Frequently Asked Questions__

 - whats' the loss in a encoder?
 - https://www.reddit.com/r/MachineLearning/comments/14y7ajc/dencoder_only_vs_encoderdecoder_vs_decoder_only/
 


### __References__

<!-- "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding": https://aclanthology.org/N19-1423.pdf -->

- __[Stanford CS224N - Winter 2021 Lecture 09 - Self-Attention and Transformers](https://www.youtube.com/watch?v=ptuGllU5SQQ)__
- __[Stanford CS224N - Winter 2021 Lecture 10 - Transformers and Pre-training](https://www.youtube.com/watch?v=j9AcEI98C0o)__
- __[Dive into Deep Learning - 11. Attention Mechanisms and Transformers](https://d2l.ai/chapter_attention-mechanisms-and-transformers/queries-keys-values.html)__
- __["Attention? Attention!" from Lilian Weng blog](https://lilianweng.github.io/posts/2018-06-24-attention/)__


http://jalammar.github.io/illustrated-transformer/

https://www.youtube.com/watch?v=rBCqOTEfxvg

https://www.youtube.com/watch?v=fEVyfT-gLqQ

https://generativeai.pub/explainable-ai-visualizing-attention-in-transformers-4eb931a2c0f8

https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html

https://www.youtube.com/watch?v=acxqoltilME


