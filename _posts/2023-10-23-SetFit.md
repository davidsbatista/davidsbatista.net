---
layout: post
title: Sentence Transformer Fine-Tuning - SetFit
date: 2023-10-22 00:00:00
tags: sentence-transformers BERT triplet-loss embeddings fine-tuning setfit
categories: [blog]
comments: true
disqus_identifier: 20231023
preview_pic: /assets/images/2023-10-23-SetFit.png
---


SetFit is a technique to fine-tune pre-trained pre-trained `sentence-transformers` on a small number of text pairs, in a contrastive Siamese manner. The resulting model is then used to generate rich text embeddings, which are used to train a classification head. 



## __Introduction__


https://nips.cc/virtual/2022/59465



SETFIT is based on Sentence Transformers (Reimers and Gurevych, 2019) which are modi- fications of pretrained transformer models that use Siamese and triplet network structures to derive se- mantically meaningful sentence embeddings. The goal of these models is to minimize the distance be- tween pairs of semantically similar sentences and maximize the distance between sentence pairs that are semantically distant.


SETFIT uses a two-step training approach in which we first fine-tune an ST and then train a classifier head. In the first step, an ST is fine-tuned on the input data in a contrastive, Siamese manner on sentence pairs. In the second step, a text classifica- tion head is trained using the encoded training data generated by the fine-tuned ST from the first step.



### __sentence-transformers fine-tuning__


ST fine-tuning To better handle the limited amount of labeled training data in few-shot scenarios, we adopt __a contrastive training approach__ that is often used for image similarity (Koch et al., 2015).

$$K$$ = set of labeled examples

$$D = { (x_{i} , y_{i}) }$$ 

pair of sentence, label, $$x_{i}$$ are sentences, $$y_{i}$$ are their class labels


For each class label $$c \in C$$, we generate a set of $$R$$ positive triplets:

$$T_{p}^{c} = {(x_{i},x_{j}, 1)}$$, 

where $$x_{i}$$ and $$x_{j}$$ are pairs of randomly chosen sentences from the same class $$c$$, i.e $$(y_{i} = y_{j} = c)$$

Similarly, we also generate a set of R negative triplets:

$$T_{n}^{c} = {(x_{i} , x_{j} , 0)}$$, 

where $$x_{i}$$ are sentences from class $$c$$ and $$x_{j}$$ are randomly chosen sentences from different classes such that $$(y_{i} = c, y_{j} \neq c)$$. 


The contrastive fine tuning data set $$T$$ is produced by concatenating the positive and negative triplets across all class labels:

$$T = { (T_{p}^{0},T{n}^{0}), (T_{p}^{1},T{n}^{1}), \ldots, (T_{p}^{|C|}, T_{n}^{|C|}) }$$ 

where pnpnpn |C| is the number of class labels, |T| = 2R|C| is the number of pairs in T and R is a hyperparameter. 


Unless stated otherwise, we used R = 20 in all the evaluations.




### __Overview__



BERT/RoBERTa/any-Transformer-based-model -> fine-tuned it using `sentence-transformers` on some generic dataset
 


SETFIT uses a two-step training approach in which we first fine-tune an ST and then train a classifier head. 

In the first step, __an ST is fine-tuned on the input data in a contrastive, Siamese manner on sentence pairs__. 

In the second step, a text classification head is trained using the encoded training data generated by the fine-tuned ST from the first step.









### __References__

Efficient Few-Shot Learning with Sentence Transformers: https://www.youtube.com/watch?v=8h27lV8v8BU&t=1405s


https://towardsdatascience.com/sentence-transformer-fine-tuning-setfit-outperforms-gpt-3-on-few-shot-text-classification-while-d9a3788f0b4e

https://lilianweng.github.io/posts/2021-05-31-contrastive/

Contrastive Learning in PyTorch - Part 1: Introduction https://www.youtube.com/watch?v=u-X_nZRsn5M