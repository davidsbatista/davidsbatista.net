---
layout: post
title: Sentence Transformer Fine-Tuning - SetFit
date: 2023-10-22 00:00:00
tags: sentence-transformers triplet-loss contrastive-learning fine-tuning
categories: [blog]
comments: true
disqus_identifier: 20231023
preview_pic: /assets/images/2023-10-23-SetFit.png
---


SetFit is a technique to fine-tune pre-trained pre-trained `sentence-transformers` on a small number of text pairs, in a contrastive Siamese manner. The resulting model is then used to generate rich text embeddings, which are then used to train a classification head, resulting in a final classifier fine-tuned to the specific dataset and mitigating the problem of few annotated samples.



## __Introduction__

<!--
SETFIT is based on Sentence Transformers (Reimers and Gurevych, 2019) which are modi- fications of pretrained transformer models that use Siamese and triplet network structures to derive se- mantically meaningful sentence embeddings. The goal of these models is to minimize the distance be- tween pairs of semantically similar sentences and maximize the distance between sentence pairs that are semantically distant.


SETFIT uses a two-step training approach in which we first fine-tune an ST and then train a classifier head. In the first step, an ST is fine-tuned on the input data in a contrastive, Siamese manner on sentence pairs. In the second step, a text classifica- tion head is trained using the encoded training data generated by the fine-tuned ST from the first step.
-->

<figure>
  <img style="width: 50%; height: 50%" src="/assets/images/2023-10-23-SetFit.png">
  <figcaption>Figure 1 - SetFit general approach.</figcaption>
</figure>


<!--
	Contrastive Learning in PyTorch - Part 1: Introduction https://www.youtube.com/watch?v=u-X_nZRsn5M
-->


### __Fine-tuning with Contrastive Learning__

adopt __a contrastive training approach__ that is often used for image similarity (Koch et al., 2015).

$$K$$ = set of labeled examples

<!--

$$D = { (x_{i} , y_{i}) }$$ 

pair of sentence, label, $$x_{i}$$ are sentences, $$y_{i}$$ are their class labels

-->

For each class label $$c \in C$$, we generate a set of $$R$$ positive triplets:


$$T_{p}^{c} = {(x_{i},x_{j}, 1)}$$


<!--
where $$x_{i}$$ and $$x_{j}$$ are pairs of randomly chosen sentences from the same class $$c$$, i.e $$(y_{i} = y_{j} = c)$$
-->

Similarly, we also generate a set of R negative triplets:


$$T_{n}^{c} = {(x_{i} , x_{j} , 0)}$$


$$x_{i}$$: sentences from class $$c$$

$$x_{j}$$: are randomly chosen sentences from different classes such that $$(y_{i} = c, y_{j} \neq c)$$. 



The contrastive fine tuning data set $$T$$ is produced by concatenating the positive and negative triplets across all class labels:

$$T = { (T_{p}^{0},T{n}^{0}), (T_{p}^{1},T{n}^{1}), \ldots, (T_{p}^{|C|}, T_{n}^{|C|}) }$$ 


$$ \vert C \vert $$ is the number of class labels


$$ \vert T \vert = 2R \vert C \vert $$ 

is the number of pairs in $$T$$ and $$R$$ is a hyperparameter. 


Unless stated otherwise, we used $R = 20$ in all the evaluations.




### __Whole Overview: from BERT to SetFit__

<figure>
  <img style="width: 95%; height: 50%" src="/assets/images/2023-10-23-SetFit-2-phases.png">
  <figcaption>Figure 2 - SetFit two phases.</figcaption>
</figure>

<!--
SETFIT uses a two-step training approach in which we first fine-tune an ST and then train a classifier head. In the first step, __an ST is fine-tuned on the input data in a contrastive, Siamese manner on sentence pairs__. 
In the second step, a text classification head is trained using the encoded training data generated by the fine-tuned ST from the first step.
-->


In retrospective, and starting with a pre-trained vanilla Transformer model (e.g.: BERT, RoBERTa), by applying the __[SBERT](blog/2023/10/22/SentenceTransformers)__ approach of fine-tuning for semantically (dis)similarity you get a fine-tuned `sentence-transformer`, which given two similar sentences it will generate embeddings that are also close in the embedding space.






### __References__

- __[Weng, Lilian. (May 2021). Contrastive representation learning. Lilâ€™Log](https://lilianweng.github.io/posts/2021-05-31-contrastive/)__

- __[Efficient Few-Shot Learning with Sentence Transformers](https://www.youtube.com/watch?v=8h27lV8v8BU&t=1405s)__

- __[Sentence Transformer Fine-Tuning post on Towards Data Science by Moshe Wasserblat](https://towardsdatascience.com/sentence-transformer-fine-tuning-setfit-outperforms-gpt-3-on-few-shot-text-classification-while-d9a3788f0b4e)__

- __[Video Presentation at the NIPS Workshop ENLSP-II](https://nips.cc/virtual/2022/59465)__