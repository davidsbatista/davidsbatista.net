---
layout: post
title: Sentence Transformer Fine-Tuning - SetFit
date: 2023-10-22 00:00:00
tags: sentence-transformers triplet-loss contrastive-learning fine-tuning
categories: [blog]
comments: true
disqus_identifier: 20231023
preview_pic: /assets/images/2023-10-23-SetFit.png
---


SetFit is a technique to mitigate the problem of few annotated samples by relying on a fine-tuning a pre-trained `sentence-transformers` model on a small number of text pairs in a contrastive learning manner. The resulting model is then used to generate rich text embeddings, which are then used to train a classification head, resulting in a final classifier fine-tuned to the specific dataset.



<figure>
  <img style="width: 95%; height: 50%" src="/assets/images/2023-10-23-SetFit-2-phases.png">
  <figcaption>Figure 1 - SetFit two phases.</figcaption>
</figure>



### __Fine-Tuning with Contrastive Learning__

The first step relies on a __sentence-transformer__ model and adapts a contrastive training approach that is often used for image similarity detection (Koch et al., 2015).

<figure>
  <img style="width: 50%; height: 50%" src="/assets/images/2023-10-23-SetFit-constrative-learning.png">
  <figcaption>Figure 1 - Contrastive Learning from Vision AI.</figcaption>
</figure>



<!--
$$K$$ = set of labeled examples

$$D = { (x_{i} , y_{i}) }$$ 

pair of sentence, label, $$x_{i}$$ are sentences, $$y_{i}$$ are their class labels

-->

For each class label $$c \in C$$, we generate a set of positive triplets and negative triples:


$$T_{p}^{c} = {(x_{i},x_{j}, 1)}$$

$$T_{n}^{c} = {(x_{i} , x_{j} , 0)}$$

<!--
where $$x_{i}$$ and $$x_{j}$$ are pairs of randomly chosen sentences from the same class $$c$$, i.e $$(y_{i} = y_{j} = c)$$
-->


$$x_{i}$$: sentences from class $$c$$

$$x_{j}$$: are randomly chosen sentences from different classes such that $$(y_{i} = c, y_{j} \neq c)$$. 


The contrastive fine tuning data set $$T$$ is produced by concatenating the positive and negative triplets across all class labels:

$$T = { (T_{p}^{0},T{n}^{0}), (T_{p}^{1},T{n}^{1}), \ldots, (T_{p}^{|C|}, T_{n}^{|C|}) }$$ 


$$ \vert C \vert $$ is the number of class labels

$$ \vert T \vert = 2R \vert C \vert $$ 

is the number of pairs in $$T$$ and $$R$$ is a hyperparameter. 



<figure>
  <img style="width: 95%; height: 50%" src="/assets/images/2023-10-23-SetFit-phase-1.png">
  <figcaption>Figure 1 - SetFit two phases.</figcaption>
</figure>



<figure>
  <img style="width: 95%; height: 50%" src="/assets/images/2023-10-23-SetFit-phase-2.png">
  <figcaption>Figure 2 - SetFit two phases.</figcaption>
</figure>





## __Training Classification Head__



<!--
SETFIT is based on Sentence Transformers (Reimers and Gurevych, 2019) which are modifications of pre-trained transformer models that use Siamese and Triplet network structures to derive se- mantically meaningful sentence embeddings. The goal of these models is to minimize the distance be- tween pairs of semantically similar sentences and maximize the distance between sentence pairs that are semantically distant.


SETFIT uses a two-step training approach in which we first fine-tune an ST and then train a classifier head. In the first step, an ST is fine-tuned on the input data in a contrastive, Siamese manner on sentence pairs. In the second step, a text classifica- tion head is trained using the encoded training data generated by the fine-tuned ST from the first step.
-->



<!--
	Contrastive Learning in PyTorch - Part 1: Introduction https://www.youtube.com/watch?v=u-X_nZRsn5M
-->






### __Overview__

In retrospective, and starting with a pre-trained vanilla Transformer model (e.g.: BERT, RoBERTa), by applying the __[SBERT](blog/2023/10/22/SentenceTransformers)__ approach of fine-tuning for semantically (dis)similarity you get a fine-tuned `sentence-transformer`, which given two similar sentences it will generate embeddings that are also close in the embedding space.


<figure>
  <img style="width: 50%; height: 50%" src="/assets/images/2023-10-23-SetFit.png">
  <figcaption>Figure 1 - SetFit general approach.</figcaption>
</figure>









### __References__

- __[Original paper: Efficient Few-Shot Learning Without Prompts (PDF)](https://neurips2022-enlsp.github.io/papers/paper_17.pdf)__

- __[Weng, Lilian. (May 2021). Contrastive representation learning. Lil’Log](https://lilianweng.github.io/posts/2021-05-31-contrastive/)__

- __[Efficient Few-Shot Learning with Sentence Transformers](https://www.youtube.com/watch?v=8h27lV8v8BU&t=1405s)__

- __[Sentence Transformer Fine-Tuning post on Towards Data Science by Moshe Wasserblat](https://towardsdatascience.com/sentence-transformer-fine-tuning-setfit-outperforms-gpt-3-on-few-shot-text-classification-while-d9a3788f0b4e)__

- __[Video Presentation at the NIPS Workshop ENLSP-II](https://nips.cc/virtual/2022/59465)__

- __[The Beginner’s Guide to Contrastive Learning from v7labs](https://www.v7labs.com/blog/contrastive-learning-guide)__
